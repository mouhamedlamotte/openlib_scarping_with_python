{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_func(url):\n",
    "    header = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0'}\n",
    "    r = requests.get(url, headers=header)\n",
    "    return BeautifulSoup(r.text, 'lxml')\n",
    "def pretiffy(soup):\n",
    "    return soup.prettify()\n",
    "def xpath(soup, path):\n",
    "    return etree.HTML(str(soup)).xpath(path)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mouhamed/projets/biblio/openlib_scarping_with_python/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openlibrary.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('https://openlibrary.org/trending/forever', verify=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/trending/forever?page=1', '/trending/forever?page=2', '/trending/forever?page=3', '/trending/forever?page=4', '/trending/forever?page=5', '/trending/forever?page=6', '/trending/forever?page=7', '/trending/forever?page=8', '/trending/forever?page=9', '/trending/forever?page=10']\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://openlibrary.org/trending/forever'\n",
    "\n",
    "soup = soup_func(URL)\n",
    "\n",
    "# pagination list pages\n",
    "pagination_links = [link['href'] for link in soup.find_all('a', attrs={'class': 'ChoosePage'})][:-1:]\n",
    "pagination_links = ['/trending/forever?page=1'] + pagination_links\n",
    "print(pagination_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jump_data(filname, data):\n",
    "    import json\n",
    "    with open(filname, 'w') as f:\n",
    "        json.dump(data, f , indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape(pagination_links):\n",
    "    # all = []\n",
    "    for pagination_link in pagination_links:\n",
    "\n",
    "        URL = f\"https://openlibrary.org{pagination_link}\"\n",
    "\n",
    "        #scarpe the actual pagination link page\n",
    "        pagination_soup = soup_func(URL)\n",
    "\n",
    "        # get the list of books of this actual page\n",
    "        list_group = pagination_soup.find_all('li', attrs={'class': 'searchResultItem'})\n",
    "\n",
    "        # insert into a dictionary format books detail\n",
    "        data = []\n",
    "\n",
    "        # get the link detail page of every book\n",
    "        links_detail_book_page = [link.find('a', attrs={'itemprop': 'url'})['href'] for link in list_group]\n",
    "        for books_detail in links_detail_book_page:\n",
    "            NEW_URL = f\"https://openlibrary.org{books_detail}\"\n",
    "            #book_detail = requests.get(NEW_URL)\n",
    "            new_soup = soup_func(NEW_URL) #BeautifulSoup(book_detail.text, 'html.parser')\n",
    "            publishers = [publisher.text for publisher in new_soup.find_all('a', attrs={'itemprop': 'publisher'})]\n",
    "            try:\n",
    "                isbn = [isbn.text for isbn in new_soup.find_all('dd', attrs={'class': 'object', 'itemprop': 'isbn'})]\n",
    "                isbn10 = isbn[0]\n",
    "                isbn15 = isbn[1]\n",
    "            except:\n",
    "                isbn10 = None\n",
    "                isbn15 = None\n",
    "            try:\n",
    "                page = new_soup.find('span', attrs={'class':'edition-pages' ,'itemprop': 'numberOfPages'}).text\n",
    "            except:\n",
    "                page = None\n",
    "            try:\n",
    "                language = [language.a.text for language in new_soup.find_all('span', attrs={'itemprop':'inLanguage'})]\n",
    "            except ValueError:\n",
    "                language = None\n",
    "            try:\n",
    "                title = new_soup.find('h1', attrs={'class':'work-title', 'itemprop':'name'}).text\n",
    "            except:\n",
    "                title = None\n",
    "            book_data = {\n",
    "                    'title': title,\n",
    "                    'author': new_soup.find('a', attrs={'itemprop': 'author'}).text, #xpath(new_soup, '//*[@id=\"contentBody\"]/div[1]/div[3]/div[2]/span/h2[2]/a'),\n",
    "                    'publication_date': xpath(new_soup, '//*[@id=\"contentBody\"]/div[1]/div[3]/div[5]/div/div[1]/span'),\n",
    "                    'page': page,\n",
    "                    'language': ' '.join(language),\n",
    "                    'publishers': f\"{', '.join(publishers)}\",\n",
    "                    'description': xpath(new_soup, '//*[@id=\"contentBody\"]/div[1]/div[3]/div[4]/div/p[1]'),\n",
    "                    'isbn10': isbn10, #xpath(new_soup, '//*[@id=\"contentBody\"]/div[1]/div[3]/div[9]/div[4]/dl/dd[4]'),\n",
    "                    'isbn15': isbn15,\n",
    "                    'image':new_soup.find('img', attrs={'itemprop': 'image'})['src'],\n",
    "                    'genre':[a.text for a in new_soup.find_all('a', attrs={'data-ol-link-track' : \"BookOverview|SubjectClick\"})]\n",
    "                    }\n",
    "            # print(book_data)    \n",
    "            data.append(book_data)\n",
    "            with open('data/books.json', 'w') as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "        # jump_data(f\"scraped_data/page_{int(pagination_link[-1]) + 1}.json\", data)\n",
    "        # all.append(data)\n",
    "    # jump_data('scraped_data/all.json', all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape(pagination_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
